{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSM HOMEWORK\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理\n",
    "遍历20个文本文件夹下的每一个文本文件，最终能够得到18828个文本文件路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import chardet\n",
    "import numpy as np\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:/PYCHARM/untitled3/20news-18828/alt.atheism\n",
      "G:/PYCHARM/untitled3/20news-18828/comp.graphics\n",
      "G:/PYCHARM/untitled3/20news-18828/comp.os.ms-windows.misc\n",
      "G:/PYCHARM/untitled3/20news-18828/comp.sys.ibm.pc.hardware\n",
      "G:/PYCHARM/untitled3/20news-18828/comp.sys.mac.hardware\n",
      "G:/PYCHARM/untitled3/20news-18828/comp.windows.x\n",
      "G:/PYCHARM/untitled3/20news-18828/misc.forsale\n",
      "G:/PYCHARM/untitled3/20news-18828/rec.autos\n",
      "G:/PYCHARM/untitled3/20news-18828/rec.motorcycles\n",
      "G:/PYCHARM/untitled3/20news-18828/rec.sport.baseball\n",
      "G:/PYCHARM/untitled3/20news-18828/rec.sport.hockey\n",
      "G:/PYCHARM/untitled3/20news-18828/sci.crypt\n",
      "G:/PYCHARM/untitled3/20news-18828/sci.electronics\n",
      "G:/PYCHARM/untitled3/20news-18828/sci.med\n",
      "G:/PYCHARM/untitled3/20news-18828/sci.space\n",
      "G:/PYCHARM/untitled3/20news-18828/soc.religion.christian\n",
      "G:/PYCHARM/untitled3/20news-18828/talk.politics.guns\n",
      "G:/PYCHARM/untitled3/20news-18828/talk.politics.mideast\n",
      "G:/PYCHARM/untitled3/20news-18828/talk.politics.misc\n",
      "G:/PYCHARM/untitled3/20news-18828/talk.religion.misc\n",
      "文档数: 18828\n"
     ]
    }
   ],
   "source": [
    "path='G:/PYCHARM/untitled3/20news-18828'\n",
    "packs=os.listdir(path)\n",
    "result1=[]\n",
    "result2=[]\n",
    "for pack in packs:\n",
    "     path1=path+\"/\"+pack\n",
    "     print(path1)\n",
    "     files=os.listdir(path1)\n",
    "     result1.append(path1)\n",
    "     for file in files:\n",
    "          path2=path1+\"/\"+file\n",
    "          result2.append(path2)\n",
    "\n",
    "print('文档数:',len(result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造单词包\n",
    "18828个文本所含信息过于庞大，为了能有效的提取出所有有效的词汇，有必要对文本中的标点、数字以及停用词进行过滤，而且考虑到许多单词同属一个词根，所以需要运用nltk模块来进行操作，以上操作过后从所有文本中提取到的单词量从18w+下降到12w+，然而这仍然是一个不小的数目，经检验，如果直接用该词汇量进行后续模型的构建，会给电脑带来巨大负担，极易引起MEMORY ERROR错误。所以，单词量必须得再一次精简。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer as ss\n",
    "stp = stopwords.words('english')\n",
    "stm = ss('english')\n",
    "symbols= [',','.',':','_','!','?','/','\\'','\\\"','*','>','<','@','~','-','(',')','%','=','\\\\','^'\n",
    "     ,'&','|','#','$','0','1','2','3','4','5','6','7','8','9','10','[',']','+','{','}',';','`','~']#需要过滤掉的各类标点及数字\n",
    "glossary={}\n",
    "wordlist={}\n",
    "vocab=[]\n",
    "for d_path in result2:\n",
    "         d = open(d_path,'rb')\n",
    "         data = d.read()\n",
    "         encode = chardet.detect(data)['encoding']\n",
    "         with codecs.open(d_path, encoding=encode) as d:\n",
    "             words = d.read()\n",
    "             for symbol in symbols:\n",
    "                 words = words.replace(symbol,'')\n",
    "             words = words.split()\n",
    "             for word in words:\n",
    "                 word = stm.stem(word)\n",
    "                 if word not in stp:\n",
    "                     if word in wordlist:\n",
    "                         wordlist[word]+=1\n",
    "                     else:\n",
    "                         wordlist[word]=1\n",
    "print(len(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128281\n",
    "\n",
    "\n",
    "\n",
    "显然词汇量还不够精简，需要考虑将出现次数过少及过高的词筛掉，以此来构建一个更小的字典。并输出包含所有有效词及其数目的文件bagpack.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in wordlist:\n",
    "    if wordlist[k] >= 4 and wordlist[k] <= 1000:\n",
    "        glossary[k]=wordlist[k]\n",
    "        vocab.append(\"%s,%s\\n\" %(k,wordlist[k]))\n",
    "\n",
    "with open('bagpack.txt','w+',encoding='utf-8') as f:\n",
    "    f.writelines(vocab)\n",
    "\n",
    "print(glossary)\n",
    "print(len(glossary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'mathew': 173, 'mathewmantiscouk': 84, 'altath': 107, 'faq': 824, 'atheist': 962, 'resourc': 721, 'archivenam': 79, 'altatheismarchivenam': 7, 'lastmodifi': 56, 'decemb': 185, 'usa': 917, 'freedom': 618, 'foundat': 229, 'darwin': 30, 'fish': 162, 'bumper': 52, 'sticker': 92, 'assort': 33, 'paraphernalia': 5, 'po': 366, 'madison': 51, 'wi': 54, 'telephon': 340, 'evolut': 147, 'symbol': 187, 'stick': 456, 'feet': 303, 'written': 772, 'insid': 587, 'delux': 41, 'mould': 6, 'plastic': 198, 'laurel': 11, 'canyon': 30, 'north': 459, 'hollywood': 27, 'ca': 804, 'san': 702, 'francisco': 244, 'bay': 235, 'lynn': 67, 'gold': 162, 'per': 876, 'aap': 15, 'publish': 765, 'various': 733, 'critiqu': 39, 'biblic': 292, 'contradict': 302, 'handbook': 79, 'wp': 43, 'ball': 471, 'gw': 44, 'foot': 203, 'pp': 373, 'isbn': 118, 'nd': 412, 'edit': 545, 'absurd': 168, 'atroc': 86, 'immor': 171, 'king': 669, 'jame': 877, 'austin': 157, 'tx': 171, 'cameron': 18, 'road': 943, 'prometheus': 13, 'haught': 4, 'holi': 403, 'horror': 95, 'east': 456, 'amherst': 16, 'street': 580, 'buffalo': 216, 'york': 706, 'altern': 565, 'newer': 166, 'older': 292, 'glenn': 101, 'ny': 405, 'africanamerican': 18, 'promot': 233, 'black': 962, 'secular': 117, 'uncov': 34, 'freethought': 5, ……}\n",
    "\n",
    "31199\n",
    "\n",
    "词量被精简到3w+，运算量大大减少，且保留了大部分重要内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = list(glossary.keys())\n",
    "D_vec=np.zeros(len(glossary))\n",
    "for d_path in result2:\n",
    "    d=open(d_path,'rb')\n",
    "    lines=d.read()\n",
    "    lines=set(lines.split())#集合元素具有互异性\n",
    "\n",
    "    for item in lines:\n",
    "        if item in keys:   #判断是否为有效词\n",
    "            D_vec[keys.index(item)] +=1\n",
    "    d.close()\n",
    "print(D_vec)                 #包含相应词语的文件数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_save ='G:/PYCHARM/untitled3/TF_IDF_VEC.npy' #向量存储位置\n",
    "idf=np.log10(int(18828)/(D_vec+1))\n",
    "\n",
    "vector=[]\n",
    "for d_path in result2:\n",
    "    d=open(d_path,'rb')\n",
    "    tf=np.zeros(len(glossary))\n",
    "    lines=d.read()\n",
    "    lines=lines.split()\n",
    "    keys_2=list(glossary.keys())\n",
    "    for item in lines:                  \n",
    "        if item in keys_2:\n",
    "            tf[keys_2.index(item)]+=1\n",
    "    tf/=len(lines)         #计算出每个文件中每一个有效词的词频tf\n",
    "    tf_idf=idf*tf          #计算出每个文件中每一个有效词的tf-idf值\n",
    "    np.save(vec_save,tf_idf)#并储存\n",
    "\n",
    "    d.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
